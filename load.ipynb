{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_if_not_exists(client, dataset_id, location=\"US\"):\n",
    "    \"\"\"Cria o dataset no BigQuery se ele não existir.\"\"\"\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    dataset_ref = bigquery.DatasetReference(client.project, dataset_id)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = location\n",
    "    try:\n",
    "        client.get_dataset(dataset_ref)  # Verifica se o dataset já existe\n",
    "        print(f\"Dataset {dataset_id} já existe.\")\n",
    "    except Exception:\n",
    "        print(f\"Dataset {dataset_id} não encontrado. Criando dataset...\")\n",
    "        client.create_dataset(dataset)\n",
    "        print(f\"Dataset {dataset_id} criado com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_if_not_exists(client, dataset_id, table_id, schema=None):\n",
    "    \"\"\"Cria a tabela no BigQuery se ela não existir.\"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)  # Tenta obter a tabela\n",
    "        print(f\"Tabela {dataset_id}.{table_id} já existe.\")\n",
    "    except Exception:\n",
    "        print(f\"Tabela {dataset_id}.{table_id} não existe. Criando tabela...\")\n",
    "        # Cria a tabela com o esquema detectado\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        table = client.create_table(table)\n",
    "        print(f\"Tabela {dataset_id}.{table_id} criada com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_schema_from_dataframe(df):\n",
    "    \"\"\"Define o esquema do BigQuery com base no DataFrame pandas.\"\"\"\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    schema = []\n",
    "    for column, dtype in zip(df.columns, df.dtypes):\n",
    "        if pd.api.types.is_integer_dtype(dtype):\n",
    "            field_type = \"INTEGER\"\n",
    "        elif pd.api.types.is_float_dtype(dtype):\n",
    "            field_type = \"FLOAT\"\n",
    "        elif pd.api.types.is_bool_dtype(dtype):\n",
    "            field_type = \"BOOLEAN\"\n",
    "        else:\n",
    "            field_type = \"STRING\"  # Inclui tipos como objetos e strings\n",
    "\n",
    "        schema.append(bigquery.SchemaField(column, field_type))\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_multiple_csv_to_bigquery(dataset_id, table_id, csv_files):\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Carrega o primeiro CSV como DataFrame para definir o esquema e criar a tabela\n",
    "    first_file_path = csv_files[0]\n",
    "    df = pd.read_csv(first_file_path)\n",
    "    df = df.astype({' Started': str, ' Ends': str})\n",
    "    schema = define_schema_from_dataframe(df)\n",
    "\n",
    "    # Cria o dataset, se ainda não existir\n",
    "    create_dataset_if_not_exists(client, dataset_id)\n",
    "\n",
    "    # Cria a tabela, se ainda não existir\n",
    "    create_table_if_not_exists(client, dataset_id, table_id, schema=schema)\n",
    "\n",
    "    # Carrega cada arquivo CSV para a mesma tabela\n",
    "    for csv_file_path in csv_files:\n",
    "        df = pd.read_csv(csv_file_path)  # Carrega o CSV em um DataFrame\n",
    "        \n",
    "        # Envia o DataFrame para o BigQuery\n",
    "        client.load_table_from_dataframe(\n",
    "            df, \n",
    "            f\"{dataset_id}.{table_id}\",\n",
    "            job_config=bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\")\n",
    "        ).result()\n",
    "        print(f\"Arquivo {csv_file_path} enviado com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset beAnalytics não encontrado. Criando dataset...\n",
      "Dataset beAnalytics criado com sucesso.\n",
      "Tabela beAnalytics.steam_sales não existe. Criando tabela...\n",
      "Tabela beAnalytics.steam_sales criada com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_19.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_4.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_11.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_2.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_15.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_6.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_5.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_10.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_14.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_12.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_18.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_1.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_3.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_7.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_16.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_9.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_17.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_13.csv enviado com sucesso.\n",
      "Arquivo raw/steamdb_sales_page_8.csv enviado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"configs/credentials.json\"\n",
    "\n",
    "csv_files = glob.glob(\"raw/*.csv\")\n",
    "dataset_id = \"beAnalytics\"\n",
    "table_id = \"steam_sales\"\n",
    "upload_multiple_csv_to_bigquery(dataset_id, table_id, csv_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beanalytics_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
